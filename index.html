<!DOCTYPE html>
<html>

<article>
  <header>
	<h1>LEARNING DEEP AND WIDE CONTEXTUAL REPRESENTATIONS USING BERT FOR STATISTICAL PARAMETRIC SPEECH SYNTHESIS</h1>
  </header>
</article>

<p><b>Authors:</b> Ya-Jie Zhang, Zhen-Hua Ling </p>

<div><b>Abstract:</b> In this paper, we propose a method of learning deep and wide contextual representations for statistical parametric speech synthesis (SPSS) using BERT, a pre-trained language representation model. Traditional acoustic models in SPSS utilize phoneme sequences and prosody labels as text input, and can not make full use of the deep linguistic representations of current and surrounding sentences. Therefore, this paper designs two context encoders, i.e., a sentence-window context encoder and a paragraph-level context encoder, to integrate the deep contextual representations extracted from multiple sentences by BERT into the Tacotron framework via an extra attention module. The parameters of BERT are pre-trained and then fine-tuned together with other components in the model. Experimental results on the Blizzard Challenge 2019 datastet show that both context encoders can reduce the prediction errors of acoustic features. The sentence-window context encoder improves the subjective performance of the baseline Tacotron model significantly.

</div>

<p><b>Paper link:</b> </p>


<head>
<meta charset="UTF-8">
<title>Speech Demo</title>
</head>

<body>


 	

	
  </p>    
  
 </body>
</html>